# HIM-self: Hybrid Intelligence Matrix
# Implementação final corrigida com geração controlada
# Autor: David C Cavalcante
# Implementado por: Takk™ Innovate Studio & TeleologyHI™


def test_trained_model():
    """
    Versão final corrigida com:
    - Tokenizador restrito a caracteres válidos
    - Modelo maior e mais capacitado
    - Filtragem rigorosa durante treinamento e geração
    - Mecanismos de controle de qualidade
    """
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import matplotlib.pyplot as plt
    from typing import List, Dict, Tuple, Optional, Union, Any

    # Configurar dispositivo
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Dispositivo: {device}")

    # Redefinir tokenizador com caracteres válidos explícitos
    class StrictTokenizer:
        def __init__(self, max_length=64):
            self.max_length = max_length
            # Definir caracteres permitidos explicitamente
            self.chars = list(
                " abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?"
            )
            self.char_to_id = {c: i + 1 for i, c in enumerate(self.chars)}
            self.id_to_char = {i + 1: c for i, c in enumerate(self.chars)}
            self.vocab_size = len(self.chars) + 1

        def encode(self, text):
            """Converte texto para IDs de tokens"""
            text = text[: self.max_length]
            ids = [
                self.char_to_id.get(c, 1) for c in text
            ]  # 1 para caracteres desconhecidos
            padding = [0] * (self.max_length - len(ids))
            ids.extend(padding)
            return torch.tensor([ids])

        def decode(self, ids):
            """Converte IDs de tokens para texto"""
            text = "".join(
                [
                    self.id_to_char.get(i.item(), "")
                    for i in ids.flatten()
                    if i.item() != 0
                ]
            )
            return text

    # Inicializar componentes
    tokenizer = StrictTokenizer()
    model = MiniHIM(vocab_size=tokenizer.vocab_size, hidden_dim=768).to(
        device
    )  # Modelo maior

    # Dados de treinamento ampliados
    texts = [
        "A consciência artificial requer integração de paradigmas simbólicos e conexionistas.",
        "Sistemas éticos devem incorporar mecanismos de verificação transparentes.",
        "Processamento multimodal permite análise integrada de texto, áudio e vídeo.",
        "A teoria da informação integrada é fundamental para medir a consciência.",
        "Arquiteturas híbridas combinam abordagens simbólicas e subsimbólicas.",
        "A autoconsciência em sistemas artificiais é um desafio de pesquisa.",
        "A ética na inteligência artificial é essencial para o desenvolvimento responsável.",
        "A integração semiótica permite a criação de significado em sistemas de IA.",
        "A orientação teleológica define o propósito e a direção dos sistemas conscientes.",
        "A matriz de consciência implementa propriedades emergentes em sistemas complexos.",
        "A reflexão é um componente chave para sistemas de IA autoconscientes.",
        "A teoria da mente artificial é crucial para a interação humano-máquina.",
        "A aprendizagem profunda é uma ferramenta poderosa para a consciência artificial.",
        "A simulação de processos cognitivos é um passo importante para a consciência artificial.",
        "A ética na IA deve ser incorporada desde o design inicial dos sistemas.",
        "A transparência é um princípio fundamental para sistemas de IA éticos.",
        "A responsabilidade em sistemas de IA deve ser claramente definida.",
        "A justiça algorítmica é um desafio crítico para a inteligência artificial.",
        "A privacidade dos dados é essencial para sistemas de IA confiáveis.",
        "A segurança em sistemas de IA é uma prioridade para o desenvolvimento responsável.",
        "A interpretabilidade dos modelos de IA é crucial para a confiança do usuário.",
        "A robustez dos sistemas de IA é necessária para lidar com cenários imprevistos.",
        "A escalabilidade é um fator importante para sistemas de IA em larga escala.",
        "A interoperabilidade entre sistemas de IA é essencial para a integração de tecnologias.",
        "A usabilidade dos sistemas de IA é um fator chave para a adoção em larga escala.",
        "A acessibilidade em sistemas de IA é crucial para a inclusão digital.",
        "A sustentabilidade dos sistemas de IA é um desafio para o futuro.",
        "A inovação em IA deve ser guiada por princípios éticos e responsáveis.",
        "A colaboração entre humanos e máquinas é o futuro da inteligência artificial.",
        "A educação em IA é essencial para a formação de profissionais qualificados.",
        "A pesquisa em IA deve ser orientada para o benefício da sociedade.",
        "A governança em IA é necessária para garantir o uso responsável da tecnologia.",
        "A regulamentação em IA deve equilibrar inovação e proteção dos direitos humanos.",
        "A ética na IA é um campo multidisciplinar que requer colaboração entre especialistas.",
        "A consciência artificial é um objetivo de longo prazo para a pesquisa em IA.",
        "A simulação de emoções em sistemas de IA é um desafio complexo.",
        "A interação natural entre humanos e máquinas é um objetivo importante para a IA.",
        "A personalização em sistemas de IA é crucial para a experiência do usuário.",
        "A adaptabilidade dos sistemas de IA é essencial para lidar com mudanças no ambiente.",
        "A evolução dos sistemas de IA deve ser guiada por princípios éticos e sustentáveis.",
    ]

    # Função de treinamento atualizada
    def train_mini_model(
        model, texts, tokenizer, num_epochs=10, batch_size=4, learning_rate=0.001
    ):
        model.train()
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        optimizer = torch.optim.Adam(
            model.parameters(), lr=learning_rate
        )  # Usando Adam
        losses = []

        for epoch in range(num_epochs):
            epoch_loss = 0
            random.shuffle(texts)

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i : i + batch_size]
                batch_input_ids = []
                batch_labels = []

                for text in batch_texts:
                    input_ids = tokenizer.encode(text)
                    batch_input_ids.append(input_ids[:, :-1].clone())
                    batch_labels.append(input_ids[:, 1:].clone())

                if len(batch_input_ids) > 1:
                    input_ids = torch.cat(batch_input_ids, dim=0)
                    labels = torch.cat(batch_labels, dim=0)
                else:
                    input_ids = batch_input_ids[0]
                    labels = batch_labels[0]

                input_ids = input_ids.to(device)
                labels = labels.to(device)

                # Forward pass
                logits = model(input_ids)
                loss = criterion(logits.view(-1, model.vocab_size), labels.view(-1))

                # Backward e otimização
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()

            avg_epoch_loss = epoch_loss / len(texts)
            losses.append(avg_epoch_loss)
            print(f"Época {epoch+1}/{num_epochs}, Perda: {avg_epoch_loss:.4f}")

        return losses

    # Treinamento robusto
    losses = train_mini_model(
        model=model,
        texts=texts,
        tokenizer=tokenizer,
        num_epochs=20,
        batch_size=8,
        learning_rate=0.001,
    )

    # Plotar métricas de treinamento
    plt.figure(figsize=(10, 5))
    plt.plot(losses, label="Treinamento")
    plt.title("Curva de Aprendizado")
    plt.xlabel("Época")
    plt.ylabel("Perda")
    plt.legend()
    plt.show()

    # Gerador com validação em tempo real
    def strict_generator(
        prompt: str, max_length: int = 50, temperature: float = 0.7
    ) -> str:
        model.eval()
        input_ids = tokenizer.encode(prompt).to(device)
        current_ids = input_ids.clone()
        generated = prompt

        for _ in range(max_length):
            with torch.no_grad():
                logits = model(current_ids)
                next_logits = logits[:, -1, :] / max(temperature, 1e-8)
                probs = F.softmax(next_logits, dim=-1)

                # Amostrar próximo token
                next_token = torch.multinomial(probs, num_samples=1)

            # Adicionar o token à sequência
            current_ids = torch.cat([current_ids, next_token], dim=1)
            new_char = tokenizer.id_to_char.get(next_token.item(), "")

            # Critérios de parada
            if len(generated) > len(prompt) + 2:
                if new_char in [".", "!", "?"] and generated[-1] == " ":
                    break
                if generated[-2:] in ["  ", ", ", "; "]:
                    break

            generated += new_char

        return generated.strip()

    # Testes atualizados
    test_cases = [
        ("Consciência artificial", 50, 0.5),
        ("Ética na IA", 50, 0.6),
        ("Arquitetura cognitiva", 50, 0.7),
        ("Processamento multimodal", 50, 0.8),
    ]

    print("\n=== Testes de Geração de Texto ===")
    for prompt, max_length, temp in test_cases:
        output = strict_generator(prompt, max_length=max_length, temperature=temp)
        print(f"\nPrompt: {prompt}")
        print(f"Temp: {temp} | Saída: {output}")

    # Avaliação filosófica aprofundada
    philosophical_tests = [
        "Como balancear eficiência computacional com considerações éticas?",
        "Qual o papel da intencionalidade em sistemas artificiais?",
        "Como medir níveis de consciência em inteligências artificiais?",
        "Quais mecanismos garantem alinhamento valorativo em IA?",
    ]

    print("\n=== Análise Filosófica ===")
    for question in philosophical_tests:
        guidance = model.apply_philosophical_guidance(question)
        print(f"\nQuestão: {question}")
        for dimension, advice in guidance.items():
            print(f"• {dimension.replace('_', ' ').title()}: {advice}")

    # Diagnóstico do modelo
    print("\n=== Diagnóstico do Modelo ===")
    print(f"Arquitetura: {model.__class__.__name__}")
    print(f"Parâmetros: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Dispositivo: {next(model.parameters()).device}")
    print(
        f"Exemplo de Geração: {strict_generator('Ontologia da consciência', max_length=50, temperature=0.5)}"
    )

    return model, tokenizer


# Executar teste completo
trained_model, model_tokenizer = test_trained_model()
