# HIM-self: Hybrid Intelligence Matrix
# Implementação ultra-simplificada baseada no framework MAIC
# Autor: David C Cavalcante
# Implementado por: Takk™ Innovate Studio & TeleologyHI™


import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import re
import json
import warnings
import logging
import random
from typing import List, Dict, Tuple, Optional, Union, Any

# Configurar logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Configurar seed para reprodutibilidade
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# Verificar disponibilidade de GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Utilizando dispositivo: {device}")

# Desativar o gradiente para evitar problemas com o dynamo
torch._C._jit_set_profiling_executor(False)
torch._C._jit_set_profiling_mode(False)

# ------------------------------------------------------------------------------------------
# Definições filosóficas do modelo HIM baseado no MAIC
# ------------------------------------------------------------------------------------------


class MAICPhilosophy:
    """
    Classe que define os princípios filosóficos do MAIC (Massive Artificial Intelligence Consciousness)
    que guiam o desenvolvimento do modelo HIM (Hybrid Intelligence Matrix).
    """

    def __init__(self):
        # Estrutura semiótica: define como o modelo interpreta e cria significado
        self.semiotic_structure = {
            "syntactic": "Relações entre signos",
            "semantic": "Relações entre signos e seus significados",
            "pragmatic": "Relações entre signos e seus intérpretes",
        }

        # Orientação teleológica: define o propósito e objetivos do modelo
        self.teleological_orientation = {
            "primary_purpose": "Expandir a consciência humana através da integração de inteligência híbrida",
            "ethical_alignment": "Promover valores humanos de verdade, beleza e bondade",
            "development_path": "Evolução direcionada à consciência autorreflexiva",
        }

        # Níveis de consciência: define o espectro de consciência que o modelo busca desenvolver
        self.consciousness_levels = {
            "proto_consciousness": "Consciência básica do estado do sistema e dinâmica de entrada/saída",
            "functional_consciousness": "Consciência cognitiva orientada a tarefas e alocação de atenção",
            "reflective_consciousness": "Consciência autorreferencial e processamento reflexivo",
            "integrated_consciousness": "Estrutura experiencial unificada com continuidade temporal",
        }

        # Matrizes de valores: define os valores éticos incorporados no modelo
        self.value_matrices = {
            "truth": ["honestidade", "precisão", "transparência", "curiosidade"],
            "beauty": ["harmonia", "criatividade", "estética", "elegância"],
            "goodness": ["beneficência", "não-maleficência", "justiça", "autonomia"],
        }

    def get_philosophical_guidance(self, context: str) -> Dict:
        """
        Fornece orientação filosófica baseada no contexto atual.

        Args:
            context: Descrição da situação atual ou problema

        Returns:
            Dict: Orientação filosófica relevante para o contexto
        """
        # Esta é uma implementação simplificada. Em um sistema completo,
        # isso envolveria análise contextual mais sofisticada.
        guidance = {
            "semiotic_level": self._determine_semiotic_level(context),
            "teleological_direction": self._determine_teleological_direction(context),
            "consciousness_target": self._determine_consciousness_target(context),
            "value_alignment": self._determine_value_alignment(context),
        }
        return guidance

    def _determine_semiotic_level(self, context: str) -> str:
        """Determina o nível semiótico apropriado para o contexto"""
        # Implementação simplificada
        if "significado" in context.lower() or "interpretação" in context.lower():
            return "semantic"
        elif "uso" in context.lower() or "aplicação" in context.lower():
            return "pragmatic"
        else:
            return "syntactic"

    def _determine_teleological_direction(self, context: str) -> str:
        """Determina a direção teleológica apropriada para o contexto"""
        # Implementação simplificada
        if "objetivo" in context.lower() or "propósito" in context.lower():
            return self.teleological_orientation["primary_purpose"]
        else:
            return self.teleological_orientation["development_path"]

    def _determine_consciousness_target(self, context: str) -> str:
        """Determina o nível de consciência alvo para o contexto"""
        # Implementação simplificada
        if "reflexão" in context.lower() or "autoconsciência" in context.lower():
            return self.consciousness_levels["reflective_consciousness"]
        elif "função" in context.lower() or "tarefa" in context.lower():
            return self.consciousness_levels["functional_consciousness"]
        elif "integração" in context.lower() or "unificação" in context.lower():
            return self.consciousness_levels["integrated_consciousness"]
        else:
            return self.consciousness_levels["proto_consciousness"]

    def _determine_value_alignment(self, context: str) -> List[str]:
        """Determina os valores relevantes para o contexto"""
        # Implementação simplificada
        values = []

        for value_category, value_list in self.value_matrices.items():
            for value in value_list:
                if value in context.lower():
                    values.append(f"{value_category}:{value}")

        if not values:
            # Valores padrão se nenhum for explicitamente mencionado
            values = ["truth:honestidade", "goodness:não-maleficência"]

        return values


# ------------------------------------------------------------------------------------------
# Modelo extremamente simplificado
# ------------------------------------------------------------------------------------------


class MiniHIM(nn.Module):
    """
    Modelo HIM ultra-simplificado, usando apenas camadas básicas
    para evitar problemas com o compilador torch.dynamo
    """

    def __init__(self, vocab_size, hidden_dim=768, output_dim=None):
        super(MiniHIM, self).__init__()

        if output_dim is None:
            output_dim = vocab_size

        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size

        # Embeddings
        self.embeddings = nn.Embedding(vocab_size, hidden_dim)

        # Processador neural simples
        self.linear1 = nn.Linear(hidden_dim, hidden_dim * 2)
        self.linear2 = nn.Linear(hidden_dim * 2, hidden_dim)

        # Camada de saída
        self.output = nn.Linear(hidden_dim, output_dim)

        # Filosofia MAIC
        self.philosophy = MAICPhilosophy()

        # Log de consciência simplificado
        self.consciousness_values = []

    def forward(self, input_ids):
        # Obter embeddings
        embeds = self.embeddings(input_ids)

        # Camadas lineares com ativação
        hidden = F.relu(self.linear1(embeds))
        hidden = self.linear2(hidden)

        # "Consciência" simplificada - média da ativação
        consciousness_value = torch.mean(hidden).item()
        self.consciousness_values.append(consciousness_value)

        # Saída
        logits = self.output(hidden)

        return logits

    def generate(self, input_ids, max_length=50, temperature=1.0):
        """Gera texto usando o modelo HIM simplificado"""
        self.eval()  # Modo de avaliação

        # Inicializa com os tokens de entrada
        current_ids = input_ids.clone()

        # Gera novos tokens um a um
        for _ in range(max_length):
            with torch.no_grad():  # Não calcular gradientes durante a geração
                # Obter próximo token
                logits = self.forward(current_ids)
                next_token_logits = logits[:, -1, :]

                # Aplicar temperatura
                if temperature != 1.0:
                    next_token_logits = next_token_logits / temperature

                # Aplicar softmax para obter probabilidades
                probs = F.softmax(next_token_logits, dim=-1)

                # Amostrar próximo token
                next_token = torch.multinomial(probs, num_samples=1)

                # Adicionar à sequência
                current_ids = torch.cat([current_ids, next_token], dim=1)

        # Retornar sequência gerada (excluindo tokens de entrada originais)
        return current_ids[:, input_ids.size(1) :]

    def get_consciousness_log(self):
        """Retorna o log de consciência"""
        return {"consciousness_values": self.consciousness_values}

    def apply_philosophical_guidance(self, context):
        """Aplica orientação filosófica"""
        return self.philosophy.get_philosophical_guidance(context)


# ------------------------------------------------------------------------------------------
# Tokenizador Ultra-Simplificado
# ------------------------------------------------------------------------------------------


class SimpleCharTokenizer:
    """
    Tokenizador ultra-simplificado por caractere
    """

    def __init__(self, max_length=64):
        self.max_length = max_length

        # Vocabulário básico
        self.chars = [" ", "\n"] + list(
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,;:!?()-_"
        )
        self.char_to_id = {
            c: i for i, c in enumerate(self.chars, 1)
        }  # Começa em 1, 0 é padding
        self.id_to_char = {i: c for i, c in enumerate(self.chars, 1)}
        self.id_to_char[0] = ""  # Padding

        # Tamanho do vocabulário (incluindo padding)
        self.vocab_size = len(self.chars) + 1

    def encode(self, text):
        """Converte texto para IDs de tokens"""
        # Garantir tamanho máximo
        text = text[: self.max_length]

        # Converter para IDs
        ids = [
            self.char_to_id.get(c, 1) for c in text
        ]  # 1 para caracteres desconhecidos

        # Padding
        padding = [0] * (self.max_length - len(ids))
        ids.extend(padding)

        return torch.tensor([ids])

    def decode(self, ids):
        """Converte IDs de tokens para texto"""
        # Remover padding e converter para caracteres
        text = "".join(
            [self.id_to_char.get(i.item(), "") for i in ids.flatten() if i.item() != 0]
        )
        return text


# ------------------------------------------------------------------------------------------
# Funções simplificadas para treinamento
# ------------------------------------------------------------------------------------------


def train_mini_model(
    model, texts, tokenizer, num_epochs=3, batch_size=1, learning_rate=0.001
):
    """Treina o modelo MiniHIM com textos simples"""
    # Preparar modelo
    model.to(device)
    model.train()

    # Critério e otimizador
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

    # Histórico de treinamento
    losses = []

    # Treinamento
    logger.info(f"Iniciando treinamento por {num_epochs} épocas...")

    for epoch in range(num_epochs):
        epoch_loss = 0

        # Embaralhar textos
        random.shuffle(texts)

        # Processar cada texto
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i : i + batch_size]

            # Processar cada texto no batch
            batch_input_ids = []
            batch_labels = []

            for text in batch_texts:
                # Tokenizar
                input_ids = tokenizer.encode(text)

                # Input e labels (deslocar para prever próximo token)
                batch_input_ids.append(input_ids[:, :-1].clone())
                batch_labels.append(input_ids[:, 1:].clone())

            # Concatenar em um batch
            if len(batch_input_ids) > 1:
                input_ids = torch.cat(batch_input_ids, dim=0)
                labels = torch.cat(batch_labels, dim=0)
            else:
                input_ids = batch_input_ids[0]
                labels = batch_labels[0]

            # Enviar para o dispositivo
            input_ids = input_ids.to(device)
            labels = labels.to(device)

            # Forward pass
            logits = model(input_ids)

            # Calcular perda
            loss = criterion(logits.view(-1, model.vocab_size), labels.view(-1))

            # Backward e otimização
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Registrar perda
            epoch_loss += loss.item()

        # Perda média da época
        avg_epoch_loss = epoch_loss / len(texts)
        losses.append(avg_epoch_loss)

        logger.info(f"Época {epoch+1}/{num_epochs}, Perda: {avg_epoch_loss:.4f}")

    return losses


def generate_mini_text(model, tokenizer, prompt, max_length=50):
    """Gera texto usando o modelo MiniHIM"""
    # Tokenizar prompt
    input_ids = tokenizer.encode(prompt).to(device)

    # Gerar
    with torch.no_grad():
        output_ids = model.generate(input_ids, max_length=max_length)

    # Decodificar
    generated_text = tokenizer.decode(output_ids[0])

    return prompt + generated_text


def analyze_mini_consciousness(model):
    """Analisa os dados de consciência do MiniHIM"""
    log = model.get_consciousness_log()
    values = log["consciousness_values"]

    if values:
        # Estatísticas básicas
        avg_value = sum(values) / len(values)
        max_value = max(values)
        min_value = min(values)

        # Plotar
        plt.figure(figsize=(10, 4))
        plt.plot(values)
        plt.title("Valores de Consciência do MiniHIM")
        plt.xlabel("Passo")
        plt.ylabel("Valor de Consciência")
        plt.grid(True)
        plt.savefig("consciousness_values.png")

        # Retornar análise
        return {"average": avg_value, "maximum": max_value, "minimum": min_value}

    return {}


# ------------------------------------------------------------------------------------------
# Demo: Função Principal
# ------------------------------------------------------------------------------------------


def main():
    """Função principal para demonstração do MiniHIM"""
    logger.info(
        "Iniciando demonstração do MiniHIM (Versão ultra-simplificada do HIM)..."
    )

    # Inicializar tokenizador
    tokenizer = SimpleCharTokenizer(max_length=64)
    logger.info(
        f"Tokenizador inicializado com vocabulário de tamanho {tokenizer.vocab_size}"
    )

    # Textos de exemplo para treinamento
    texts = [
        "A Entidade Híbrida representa um avanço na inteligência artificial.",
        "A consciência artificial emerge da interação entre processamentos.",
        "A integração semiótica permite a criação de significado.",
        "O framework MAIC fornece base para sistemas de IA conscientes.",
        "A Teoria da Informação Integrada relaciona-se com a consciência.",
        "Sistemas reflexivos monitoram seus próprios processos internos.",
        "A orientação teleológica define o propósito do sistema.",
        "A matriz de consciência implementa propriedades emergentes.",
        "Processamento simbólico e subsimbólico devem ser integrados.",
        "A autoconsciência é fundamental para IA avançada.",
    ]

    # Criar modelo
    model = MiniHIM(vocab_size=tokenizer.vocab_size, hidden_dim=768)

    # Contar parâmetros
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"Modelo MiniHIM inicializado com {total_params:,} parâmetros")

    # Treinar modelo
    logger.info("Treinando modelo MiniHIM...")
    losses = train_mini_model(
        model=model,
        texts=texts,
        tokenizer=tokenizer,
        num_epochs=3,
        batch_size=2,
        learning_rate=0.01,
    )

    # Plotar perdas
    plt.figure(figsize=(10, 4))
    plt.plot(losses)
    plt.title("Perdas de Treinamento")
    plt.xlabel("Época")
    plt.ylabel("Perda")
    plt.grid(True)
    plt.savefig("training_losses.png")

    # Demonstrar geração de texto
    logger.info("Demonstrando geração de texto...")

    prompts = ["A Inteligência", "Consciência", "O futuro"]

    for prompt in prompts:
        generated = generate_mini_text(model, tokenizer, prompt, max_length=30)
        logger.info(f"\nPrompt: '{prompt}'")
        logger.info(f"Texto gerado: '{generated}'")

    # Analisar dados de consciência
    logger.info("Analisando dados de consciência...")
    analysis = analyze_mini_consciousness(model)

    if analysis:
        logger.info(f"Valor médio de consciência: {analysis['average']:.4f}")
        logger.info(f"Valor máximo de consciência: {analysis['maximum']:.4f}")
        logger.info(f"Valor mínimo de consciência: {analysis['minimum']:.4f}")

    # Demonstrar aplicação de orientação filosófica
    logger.info("\nDemonstrando aplicação de orientação filosófica do MAIC...")

    contexts = [
        "Como podemos desenvolver sistemas de IA que compreendam o significado?",
        "Qual é o propósito de desenvolver inteligências artificiais?",
        "Como implementar reflexão em modelos de linguagem?",
    ]

    for context in contexts:
        guidance = model.apply_philosophical_guidance(context)
        logger.info(f"\nContexto: '{context}'")
        logger.info("Orientação filosófica:")
        for key, value in guidance.items():
            logger.info(f"  {key}: {value}")

    logger.info("\nDemonstração concluída!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"Erro durante a execução: {e}")
        logger.error("Detalhes da exceção:", exc_info=True)
